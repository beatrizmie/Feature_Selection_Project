{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção de características para aprovação de crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from csv import reader\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixar a semente do gerador de números aleatórios\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar todas as colunas do dataset\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explorando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# carregando o dataset\n",
    "df = pd.read_csv(\"datasets/application_data.csv\", sep=\",\", encoding=\"latin-1\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NAME_CONTRACT_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"CODE_GENDER\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"FLAG_OWN_CAR\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"FLAG_OWN_REALTY\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Limpando o dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Substituindo valores NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_null_values = []\n",
    "    \n",
    "for column in df:\n",
    "    if df[column].isnull().any():\n",
    "        columns_with_null_values.append(column)\n",
    "\n",
    "len(columns_with_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_with_null_values].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = []\n",
    "columns_to_analyze = []\n",
    "\n",
    "for column in columns_with_null_values:\n",
    "    if df[column].isnull().sum() > len(df)/2:\n",
    "        drop_columns.append(column)\n",
    "    else:\n",
    "        columns_to_analyze.append(column)\n",
    "        \n",
    "len(drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_before_drop = len(df.columns)\n",
    "\n",
    "df = df.drop(columns = drop_columns)\n",
    "total_after_drop = len(df.columns)\n",
    "\n",
    "print(\"{0} - {1} = {2}\".format(total_before_drop, len(drop_columns), total_after_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[columns_to_analyze].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_to_analyze].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas com colunas de moda e mediana\n",
    "mode_list = []\n",
    "median_list = []\n",
    "\n",
    "for column in columns_to_analyze:\n",
    "    suffix = column[-4:]\n",
    "    if suffix == \"MODE\":\n",
    "        mode_list.append(column)\n",
    "    elif suffix == \"MEDI\":\n",
    "        median_list.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODA\n",
    "mode_list.remove('EMERGENCYSTATE_MODE')\n",
    "\n",
    "for column in mode_list:\n",
    "    df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "    columns_to_analyze.remove(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MEDIANA\n",
    "for column in median_list:\n",
    "    df[column] = df[column].fillna(df[column].median())\n",
    "\n",
    "    columns_to_analyze.remove(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista com colunas médias\n",
    "avg_list = []\n",
    "\n",
    "for column in columns_to_analyze:\n",
    "    suffix = column[-3:]\n",
    "    prefix = column[:3]\n",
    "    if suffix == \"AVG\" or prefix == \"AMT\" or prefix == \"EXT\" or df[column].isnull().sum() < 1050:\n",
    "        avg_list.append(column)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MÉDIA\n",
    "for column in avg_list:\n",
    "    df[column] = df[column].fillna(df[column].mean())\n",
    "    columns_to_analyze.remove(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_to_analyze].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NAME_TYPE_SUITE'] = df['NAME_TYPE_SUITE'].fillna('Unaccompanied')\n",
    "df['EMERGENCYSTATE_MODE'] = df['EMERGENCYSTATE_MODE'].fillna('Undefined')\n",
    "df['OCCUPATION_TYPE'] = df['OCCUPATION_TYPE'].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Removendo colunas irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_data = []\n",
    "\n",
    "for column in df.columns:\n",
    "    if column[:3] == 'EXT':\n",
    "        remove_data.append(column)\n",
    "    elif column[:8] == 'FLAG_DOC':\n",
    "        remove_data.append(column)\n",
    "        \n",
    "len(remove_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=remove_data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Convertendo categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dividindo conjuntos de teste e treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"{} train + {} test\".format(len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlação entre as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_color(value):\n",
    "    if value == 1:\n",
    "        color = 'gold'\n",
    "    elif abs(value) > 0.75:\n",
    "        color = 'royalblue'\n",
    "    elif value < 0:\n",
    "        color = 'red'\n",
    "    else:\n",
    "        color = 'green'\n",
    "    return 'color: %s' % color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "correlation = train_set.corr().style.applymap(correlation_color)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Separando a variável dependente: TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set.drop(columns=[\"TARGET\"])\n",
    "y_train = train_set[\"TARGET\"]\n",
    "x_test = test_set.drop(columns=[\"TARGET\"])\n",
    "y_test = test_set[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ajustando variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categoricas = list(x_train.select_dtypes('object').columns)\n",
    "categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "encoder = OneHotEncoder(categories=\"auto\")\n",
    "\n",
    "cat_1hot = encoder.fit_transform(x_train[categoricas])\n",
    "cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_num = x_train.drop(columns=x_train[categoricas], axis=1)\n",
    "x_test_num = x_test.drop(columns=x_test[categoricas], axis=1)\n",
    "x_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "x_train_num_tr = num_pipeline.fit_transform(x_train_num)\n",
    "x_test_num_tr = num_pipeline.fit_transform(x_test_num)\n",
    "\n",
    "x_train_num_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para juntar a numerica com a categorica\n",
    "num_attribs_train = list(x_train_num)\n",
    "num_attribs_test = list(x_test_num)\n",
    "\n",
    "full_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", num_pipeline, num_attribs_train),\n",
    "        (\"cat\", OneHotEncoder(sparse=False), categoricas),\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_pipeline_test = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", num_pipeline, num_attribs_test),\n",
    "        (\"cat\", OneHotEncoder(sparse=False), categoricas),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# x_train_prepared = full_pipeline.fit_transform(x_train)\n",
    "# x_test_prepared = full_pipeline_test.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.to_csv(\"datasets/train_dataset.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(msg, pipeline, X_train, y_train, X_test, y_test):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    model = pipeline.named_steps['lin_reg']\n",
    "    print('{}: \\nintercept = {},\\ncoefs = {}'.format(msg, model.intercept_, model.coef_))\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    RMSE = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "    print('RMSE: {}'.format(RMSE))\n",
    "    print()\n",
    "    \n",
    "alpha = 1e-3\n",
    "\n",
    "# Test o fit da regularização lasso.\n",
    "reg_lasso = Pipeline([\n",
    "        (\"std_scaler\", StandardScaler()), \n",
    "        (\"lin_reg\", Lasso(alpha=alpha))\n",
    "    ])\n",
    "experiment('Regularização Lasso', reg_lasso, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_short = x_train[0:5000]\n",
    "x_test_short = x_test[0:5000]\n",
    "y_train_short = y_train[0:5000]\n",
    "y_test_short = y_test[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y_train_short,x_train_short[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(x_test_short[list(feature_set)]) - y_test_short) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in x_train_short.columns if p not in predictors]\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,len(x_train_short.columns)+1):    \n",
    "    models_fwd.loc[i] = forward(predictors)\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_fwd.loc[10, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(predictors):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_bwd = pd.DataFrame(columns=[\"RSS\", \"model\"], index = range(1,len(x_train_short.columns)))\n",
    "\n",
    "tic = time.time()\n",
    "predictors = x_train_short.columns\n",
    "\n",
    "while(len(predictors) > 1):  \n",
    "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
    "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_bwd.loc[10, \"model\"].summary().)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\")\n",
    "print(\"Foward Selection:\")\n",
    "print(\"-----------------\")\n",
    "print(models_fwd.loc[10, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------\")\n",
    "print(\"Backward Selection:\")\n",
    "print(\"-------------------\")\n",
    "print(models_bwd.loc[10, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
